{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import statsmodels.api as sm\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         summarize)\n",
    "from ISLP import confusion_table\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv(\"../data/card_transdata.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "# Visualize the correlation matrix\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot the distribution of all variables\n",
    "df.hist(bins=20, figsize=(10, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the multiple regression on multiple columns which seem to have a correlation with the output \n",
    "y = df['fraud']\n",
    "X = MS(['distance_from_home','distance_from_last_transaction','ratio_to_median_purchase_price','repeat_retailer','used_chip','used_pin_number','online_order']).fit_transform(df) \n",
    "#X = df.drop('fraud', axis='columns')\n",
    "#X = sm.add_constant(X)\n",
    "model1 = sm.OLS(y, X)\n",
    "results1 = model1.fit()\n",
    "summarize(results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform regression with interaction between two fields to test their combined effect\n",
    "model = sm.OLS.from_formula('fraud ~ distance_from_home * distance_from_last_transaction', data=df)\n",
    "result = model.fit()\n",
    "\n",
    "# Print the summary of the regression\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform regression with variable interactions between multiple variables\n",
    "model = sm.OLS.from_formula('fraud ~ distance_from_last_transaction * ratio_to_median_purchase_price* distance_from_home* used_chip', data=df)\n",
    "result = model.fit()\n",
    "\n",
    "# Print the summary of the regression\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select predictors (excluding the last column)\n",
    "predictors = df.iloc[:, :-1]\n",
    "# Standardize the predictors\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "predictors_standardized = pd.DataFrame(scaler.fit_transform(predictors), columns=predictors.columns)\n",
    "\n",
    "# Display the head of the standardized predictors\n",
    "print(predictors_standardized.head())\n",
    "# Create a random vector of True and False values\n",
    "np.random.seed(4)\n",
    "split = np.random.choice([True, False], size=len(predictors_standardized), replace=True, p=[0.75, 0.25])\n",
    "\n",
    "# Define the training set for X (predictors)\n",
    "training_X = predictors_standardized[split]\n",
    "\n",
    "# Define the training set for Y (response)\n",
    "training_Y = df.loc[split, 'fraud']\n",
    "\n",
    "# Define the testing set for X (predictors)\n",
    "testing_X = predictors_standardized[~split]\n",
    "\n",
    "# Define the testing set for Y (response)\n",
    "testing_Y = df.loc[~split, 'fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try the KNN model and view the confusion table results\n",
    "knn = KNeighborsClassifier(n_neighbors =3)\n",
    "knn_fit=knn.fit(training_X,training_Y)\n",
    "knn_pred = knn.predict(testing_X)\n",
    "confusion_table(knn_pred,testing_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the prediction accuracy\n",
    "prediction_accuracy = knn.score(testing_X,testing_Y)\n",
    "print(prediction_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with imbalanced data\n",
    "\n",
    "We can see that KNN was really effective, however we still fail to detect 204/21906 cases of fraud, almost 1 in 100. \n",
    "\n",
    "Can we get closer to understanding the predictors of credit card fraud by balancing out the data through subsampling? Subsampling will repeat the analysis using a dataframe that is 50/50 fraud and not-fraud.\n",
    "\n",
    "This will help us enhance the fraud signal and create a more proactive fraud catching model. It will also help clarify the direction and relative magnitude of correlations between fraud and the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = 5\n",
    "np.random.seed(s) \n",
    "\n",
    "df_fraud = df.loc[df.fraud==True]\n",
    "df_full_notfraud = df.loc[df.fraud==False]\n",
    "undersplit = np.random.choice([True, False], size=len(df_full_notfraud), replace=True, p=[0.5, 0.5])\n",
    "df_under_notfraud = df_full_notfraud[undersplit][:len(df_fraud)]\n",
    "\n",
    "df_subsample_0 = pd.concat([df_under_notfraud,df_fraud])\n",
    "df_subsample = df_subsample_0.sample(frac=1, random_state=s)\n",
    "\n",
    "print(len(df_subsample) == 2*len(df_fraud))\n",
    "df_subsample\n",
    "\n",
    "df_subsample.to_csv('subsample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the data frame ```df_subsample``` to work with. ```df_subsample``` has data containing all of the frauds from the original dataset as well as an equal number of randomly selected non-frauds. If we change the seed parameter s, then an entirely different set of non-frauds can be drawn. This can help us to vary the outcomes of the experiment which will allow us to bootstrap measure the standard error of our correlations and our predicted accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=ax1)\n",
    "ax1.set_title(\"Original Correlation Matrix)\", fontsize=14)\n",
    "\n",
    "corr_subsample_matrix = df_subsample.corr()\n",
    "sns.heatmap(corr_subsample_matrix, annot=True, cmap='coolwarm', ax=ax2)\n",
    "ax2.set_title('Subsampled Correlation Matrix)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shown above are the correclation matrices for our dataframes. Wee see the signal of our correlations has been enhanced. We might expect that fitting on this subsample and testing on the full_sample we will reduce the false_negative (not fraud labels applied to frauds) by a factor of 1/50. By the same logic this validation should increase the false positives (fraud labels applied to non_frauds) 50times. We would then have a model that reduces undetected frauds from 1 missed fraud in 100 frauds to 1 missed fraud in 4,000 frauds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'subsample.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define feature columns and target column\n",
    "feature_columns = [\n",
    "    'distance_from_home', \n",
    "    'distance_from_last_transaction',\n",
    "    'ratio_to_median_purchase_price', \n",
    "    'repeat_retailer', \n",
    "    'used_chip', \n",
    "    'used_pin_number', \n",
    "    'online_order'\n",
    "]\n",
    "target_column = 'fraud'\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data[feature_columns]\n",
    "y = data[target_column]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the feature columns\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an analysis of a logistic regression model applied to a dataset for detecting fraudulent transactions. The dataset includes features such as the distance from home, distance from the last transaction, the ratio to median purchase price, repeat retailer status, chip usage, pin number usage, and whether the order was made online. The target variable is a binary indicator of fraud. The model achieved an accuracy of 94.16%, indicating strong performance in identifying fraudulent transactions.\n",
    "\n",
    "Detailed Explanation\n",
    "1. Dataset Overview\n",
    "The dataset consists of the following features:\n",
    "\n",
    "distance_from_home: The distance of the transaction location from the cardholder's home.\n",
    "distance_from_last_transaction: The distance from the location of the last transaction.\n",
    "ratio_to_median_purchase_price: The ratio of the transaction amount to the median purchase price.\n",
    "repeat_retailer: A binary variable indicating if the retailer has been visited before.\n",
    "used_chip: A binary variable indicating if the transaction used a chip.\n",
    "used_pin_number: A binary variable indicating if a PIN number was used.\n",
    "online_order: A binary variable indicating if the transaction was made online.\n",
    "fraud: The target variable, indicating whether the transaction was fraudulent (1) or not (0).\n",
    "\n",
    "2. Data Preprocessing\n",
    "The dataset was split into features (X) and target (y).\n",
    "The data was divided into training and testing sets, with 70% used for training and 30% for testing.\n",
    "Feature scaling was applied using StandardScaler to standardize the features.\n",
    "\n",
    "3. Model Training and Evaluation\n",
    "A logistic regression model was trained on the standardized training data. The model's performance was evaluated on the test set, yielding the following results:\n",
    "\n",
    "Accuracy: 94.16%\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "\n",
    "[[24264  1712]\n",
    " [ 1351 25115]]\n",
    "\n",
    "\n",
    "True Negatives (TN): 24,264 transactions correctly identified as non-fraudulent.\n",
    "False Positives (FP): 1,712 transactions incorrectly identified as fraudulent.\n",
    "False Negatives (FN): 1,351 transactions incorrectly identified as non-fraudulent.\n",
    "True Positives (TP): 25,115 transactions correctly identified as fraudulent.\n",
    "Classification Report:\n",
    "\n",
    "\n",
    "Precision:\n",
    "Class 0 (Non-fraud): 95%\n",
    "Class 1 (Fraud): 94%\n",
    "\n",
    "Recall:\n",
    "Class 0 (Non-fraud): 93%\n",
    "Class 1 (Fraud): 95%\n",
    "\n",
    "F1-score:\n",
    "Class 0 (Non-fraud): 94%\n",
    "Class 1 (Fraud): 94%\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The logistic regression model demonstrates strong performance with an accuracy of 94.16%. High precision and recall for both fraudulent and non-fraudulent transactions indicate that the model effectively minimizes false positives and false negatives. This balance ensures reliable identification of fraud, which is critical for practical applications in fraud detection.\n",
    "\n",
    "The analysis indicates that the model is well-suited for detecting fraudulent transactions based on the provided features. Future work could explore additional features, advanced modeling techniques, or further data preprocessing steps to enhance performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
